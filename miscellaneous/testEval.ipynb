{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from src import hrNetVideo\n",
    "from library.GeneralOps import writeYUV420, runTerminalCmd\n",
    "from src.auxFunctions import reconstruct, deconstruct, readFrames\n",
    "from library import hrNet\n",
    "import tensorflow as tf\n",
    "from src.rgb2yuv_yuv2rgb import RGB2YUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseRefPath = '/data/nwaliv/trainVideoSetRef/'\n",
    "# baseCompPath = '/data/nwaliv/trainVideoSetDeg/'\n",
    "baseRefPath = '/ref/'\n",
    "baseCompPath = '/alpha0.01/'\n",
    "\n",
    "DATAFRAME = pd.read_csv('dataFrames/trainVideoSet_perframeCambi_maxFrame.csv')\n",
    "DATAFRAME = DATAFRAME[DATAFRAME['FrameNum'] != 0]\n",
    "DATAFRAME = DATAFRAME[DATAFRAME['FrameNum'] != DATAFRAME['MaxFrame']]\n",
    "DATAFRAME = DATAFRAME.reset_index(drop=True)\n",
    "SAMPLE = DATAFRAME.sample(n=5, replace=False,random_state=10)\n",
    "SAMPLE = SAMPLE.reset_index(drop=True)\n",
    "\n",
    "for idx, _row in SAMPLE.iterrows():\n",
    "    # get height/width of original video\n",
    "    _height = _row['Height']\n",
    "    _width = _row['Width']\n",
    "    _frameNum = _row['FrameNum']\n",
    "    _comp = os.path.join(baseCompPath,'res_'+ _row['Comp'])\n",
    "    _ref = os.path.join(baseRefPath, _row['Ref'])\n",
    "    print(_comp)\n",
    "\n",
    "    \n",
    "    #hrNetModel = hrNetVideo(_comp,(_width,_height),_row['MaxFrame'],modelWeights='modelWeights/287_HRNET.h5')\n",
    "    #resFrame = hrNetModel.restore_frame(t=1)\n",
    "    #_ = hrNetModel.restore_video()\n",
    "    # instead change save video to return yuv frames instead and save outside of function\n",
    "    #writeYUV420('output'+_comp,_,_,_,downsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1.0, 0.01, 0.001, 0.0001, 1e-05, 5e-06, 2.5e-06, 1.5e-06 ,1e-06]\n",
    "for _alpha in alphas:\n",
    "    baseRefPath = '/ref/'\n",
    "    baseCompPath = f'/alpha{_alpha}/'\n",
    "    df = pd.read_json(f\"results/HRNET_CAMBI_ALPHA{_alpha}.json\")\n",
    "\n",
    "    DATAFRAME = pd.read_csv('dataFrames/trainVideoSet_perframeCambi_maxFrame.csv')\n",
    "    DATAFRAME = DATAFRAME[DATAFRAME['FrameNum'] != 0]\n",
    "    DATAFRAME = DATAFRAME[DATAFRAME['FrameNum'] != DATAFRAME['MaxFrame']]\n",
    "    DATAFRAME = DATAFRAME.reset_index(drop=True)\n",
    "    SAMPLE = DATAFRAME.sample(n=10, replace=False,random_state=0)\n",
    "    SAMPLE = SAMPLE.reset_index(drop=True)\n",
    "\n",
    "    df_peralpha = {\n",
    "    'Ref': [],\n",
    "    'Comp': [],\n",
    "    'Height': [],\n",
    "    'Width': [],\n",
    "    'PSNR_Y': [],\n",
    "    'PSNR_Cb': [],\n",
    "    'PSNR_Cr': [],\n",
    "    'PSNR_Y': [],\n",
    "    'SSIM': [],\n",
    "    'VMAF': [],\n",
    "    'CAMBI': []\n",
    "    }\n",
    "\n",
    "    for idx, _row in SAMPLE.iterrows():\n",
    "        # get height/width of original video\n",
    "        _height = _row['Height']\n",
    "        _width = _row['Width']\n",
    "        _frameNum = _row['FrameNum']\n",
    "        _comp = os.path.join(baseCompPath,'res_'+_row['Comp'])\n",
    "        _ref = os.path.join(baseRefPath, _row['Ref'])\n",
    "        #print(_comp)\n",
    "        runTerminalCmd(f\"vmaf -r {_ref} -d {_comp} -w {_width} -h {_height} -p 420 -b 8 --csv -o processing/evals.csv --feature psnr --feature float_ssim --feature cambi -q\")\n",
    "        cambi = pd.read_csv('processing/evals.csv')['cambi'].mean()\n",
    "        psnr_y = pd.read_csv('processing/evals.csv')['psnr_y'].mean()\n",
    "        psnr_cb = pd.read_csv('processing/evals.csv')['psnr_cb'].mean()\n",
    "        psnr_cr = pd.read_csv('processing/evals.csv')['psnr_cr'].mean()\n",
    "        ssim = pd.read_csv('processing/evals.csv')['ssim'].mean()\n",
    "        vmaf = pd.read_csv('processing/evals.csv')['vmaf'].mean()\n",
    "        df_peralpha['Ref'].append(_ref)\n",
    "        df_peralpha['Comp'].append('res'+_comp)\n",
    "        df_peralpha['Height'].append(_height)\n",
    "        df_peralpha['Width'].append(_width)\n",
    "        df_peralpha['PSNR_Y'].append(_psnr_y)\n",
    "        df_peralpha['PSNR_Cb'].append(_psnr_cb)\n",
    "        df_peralpha['PSNR_Cr'].append(_psnr_cr)\n",
    "        df_peralpha['SSIM'].append(_ssim)\n",
    "        df_peralpha['VMAF'].append(_vmaf)\n",
    "        df_peralpha['CAMBI'].append(_cambi)\n",
    "    \n",
    "    df_peralpha = pd.DataFrame(df_peralpha)\n",
    "    df_peralpha.to_csv(f'alpha{_alpha}/alphaScores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from src import hrNet\n",
    "from src.auxFunctions import reconstruct, deconstruct, readFrames\n",
    "import tensorflow as tf\n",
    "from library.GeneralOps import writeYUV420\n",
    "from src.rgb2yuv_yuv2rgb import RGB2YUV\n",
    "\n",
    "baseRefPath = '/data/nwaliv/trainVideoSetRef/'\n",
    "baseCompPath = '/data/nwaliv/trainVideoSetDeg/'\n",
    "alphas = [0.01, 0.001, 0.0001, 1e-05, 5e-06, 2.5e-06, 1.5e-06 ,1e-06]\n",
    "generator = hrNet(2, [32, 64, 128, 256], 5).model()\n",
    "for _alpha in alphas:\n",
    "    print(\"alpha = \", _alpha)\n",
    "    df = pd.read_json(f\"results/HRNET_CAMBI_ALPHA{_alpha}.json\")\n",
    "    generator.load_weights(f'results/HRNET_CAMBI_ALPHA{_alpha}.h5')\n",
    "\n",
    "    DATAFRAME = pd.read_csv('dataFrames/trainVideoSet_perframeCambi_maxFrame.csv')\n",
    "    DATAFRAME = DATAFRAME[DATAFRAME['FrameNum'] != 0]\n",
    "    DATAFRAME = DATAFRAME[DATAFRAME['FrameNum'] != DATAFRAME['MaxFrame']]\n",
    "    DATAFRAME = DATAFRAME.reset_index(drop=True)\n",
    "    SAMPLE = DATAFRAME.sample(n=5, replace=False,random_state=10)\n",
    "    SAMPLE = SAMPLE.reset_index(drop=True)\n",
    "\n",
    "    for idx, _row in SAMPLE.iterrows():\n",
    "        # get height/width of original video\n",
    "        _height = _row['Height']\n",
    "        _width = _row['Width']\n",
    "        _frameNum = _row['FrameNum']\n",
    "        _comp = os.path.join(baseCompPath, _row['Comp'])\n",
    "        _ref = os.path.join(baseRefPath, _row['Ref'])\n",
    "        print(_comp)\n",
    "\n",
    "        restoredY = np.empty((_row['MaxFrame'],_height,_width))\n",
    "        restoredU = np.empty((_row['MaxFrame'],_height,_width))\n",
    "        restoredV = np.empty((_row['MaxFrame'],_height,_width))\n",
    "        restoredFrames = np.empty((_row['MaxFrame'],_height,_width,3))\n",
    "\n",
    "        for frame in range(_row['MaxFrame']):\n",
    "            print(\"Frame \", frame)\n",
    "            frame_tmin1,frame_t,frame_tplus1 = readFrames(_comp,(_width,_height),frame,_row['MaxFrame'])\n",
    "\n",
    "            window_tmin1 = deconstruct(frame_tmin1,192)\n",
    "            window_t = deconstruct(frame_t,192)\n",
    "            window_tplus1 = deconstruct(frame_tplus1,192)\n",
    "\n",
    "            inputPatches = np.concatenate((window_tmin1, window_t, window_tplus1), axis=-1)\n",
    "            inputPatches = np.expand_dims(inputPatches, axis=0)\n",
    "            numPatches = inputPatches.shape[1]\n",
    "            inputPatches = tf.convert_to_tensor(inputPatches)\n",
    "\n",
    "            outputPatches = np.empty((1,numPatches,192,192,3))\n",
    "            for patch in range(numPatches):\n",
    "                outputPatches[:,patch,:,:,:] = generator(inputPatches[:,patch,:,:,:],training=False)\n",
    "\n",
    "            outputFrame = reconstruct(outputPatches[0],frame_t,192)\n",
    "            restoredFrames[frame,:,:,:] = outputFrame[:,:,:]\n",
    "\n",
    "            outputFrameYUV = RGB2YUV(outputFrame)\n",
    "\n",
    "            Yout = outputFrameYUV[:,:,0]\n",
    "            Uout = outputFrameYUV[:,:,1]\n",
    "            Vout = outputFrameYUV[:,:,2]\n",
    "\n",
    "            restoredY[frame,:,:] = Yout\n",
    "            restoredU[frame,:,:] = Uout\n",
    "            restoredV[frame,:,:] = Vout\n",
    "\n",
    "        savePath = f'alpha{(_alpha)}/'   \n",
    "        writeYUV420(savePath+\"res_\" +_row['Comp'],np.uint8(restoredY),np.uint8(restoredU),np.uint8(restoredV),downsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseRefPath = '/bandon/nwaliv/testVideoSet/'\n",
    "baseCompPath = '/bandon/nwaliv/adabandVideos/'\n",
    "\n",
    "\n",
    "df_adaband = {\n",
    "    'Ref': [],\n",
    "    'Comp': [],\n",
    "    'Height': [],\n",
    "    'Width': [],\n",
    "    'PSNR_Y': [],\n",
    "    'PSNR_Cb': [],\n",
    "    'PSNR_Cr': [],\n",
    "    'PSNR_Y': [],\n",
    "    'SSIM': [],\n",
    "    'VMAF': [],\n",
    "    'CAMBI': []\n",
    "    }\n",
    "\n",
    "DATAFRAME = pd.read_csv('dataFrames/testVideoSet.csv')\n",
    "for _idx, _row in DATAFRAME.iterrows():\n",
    "    _height = _row['Height']\n",
    "    _width = _row['Width']\n",
    "    _comp = os.path.join(baseCompPath,'adaband_'+_row['Comp'])\n",
    "    _ref = os.path.join(baseRefPath, _row['Ref'])\n",
    "    runTerminalCmd(f\"vmaf -r {_ref} -d {_comp} -w {_width} -h {_height} -p 420 -b 8 --csv -o processing/evals.csv --feature psnr --feature float_ssim --feature cambi -q\")\n",
    "    cambi = pd.read_csv('processing/evals.csv')['cambi'].mean()\n",
    "    psnr_y = pd.read_csv('processing/evals.csv')['psnr_y'].mean()\n",
    "    psnr_cb = pd.read_csv('processing/evals.csv')['psnr_cb'].mean()\n",
    "    psnr_cr = pd.read_csv('processing/evals.csv')['psnr_cr'].mean()\n",
    "    ssim = pd.read_csv('processing/evals.csv')['ssim'].mean()\n",
    "    vmaf = pd.read_csv('processing/evals.csv')['vmaf'].mean()\n",
    "    df_adaband['Ref'].append(_ref)\n",
    "    df_adaband['Comp'].append('res'+_comp)\n",
    "    df_adaband['Height'].append(_height)\n",
    "    df_adaband['Width'].append(_width)\n",
    "    df_adaband['PSNR_Y'].append(_psnr_y)\n",
    "    df_adaband['PSNR_Cb'].append(_psnr_cb)\n",
    "    df_adaband['PSNR_Cr'].append(_psnr_cr)\n",
    "    df_adaband['SSIM'].append(_ssim)\n",
    "    df_adaband['VMAF'].append(_vmaf)\n",
    "    df_adaband['CAMBI'].append(_cambi)\n",
    "    \n",
    "df_adaband = pd.DataFrame(df_adaband)\n",
    "df_adaband.to_csv('dataFrames/adabandScores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from library.GeneralOps import runTerminalCmd\n",
    "\n",
    "alphas = [1.0, 0.01, 0.001, 0.0001, 1e-05, 5e-06, 2.5e-06, 1.5e-06 ,1e-06]\n",
    "for _alpha in alphas:\n",
    "    baseRefPath = 'ref/'\n",
    "    baseCompPath = f'alpha{_alpha}/'\n",
    "    df = pd.read_json(f\"results/HRNET_CAMBI_ALPHA{_alpha}.json\")\n",
    "\n",
    "    DATAFRAME = pd.read_csv('dataFrames/trainVideoSet_perframeCambi_maxFrame.csv')\n",
    "    DATAFRAME = DATAFRAME[DATAFRAME['FrameNum'] != 0]\n",
    "    DATAFRAME = DATAFRAME[DATAFRAME['FrameNum'] != DATAFRAME['MaxFrame']]\n",
    "    DATAFRAME = DATAFRAME.reset_index(drop=True)\n",
    "    SAMPLE = DATAFRAME.sample(n=5, replace=False,random_state=10)\n",
    "    SAMPLE = SAMPLE.reset_index(drop=True)\n",
    "\n",
    "    df_peralpha = {\n",
    "    'Ref': [],\n",
    "    'Comp': [],\n",
    "    'Height': [],\n",
    "    'Width': [],\n",
    "    'PSNR_Y': [],\n",
    "    'PSNR_Cb': [],\n",
    "    'PSNR_Cr': [],\n",
    "    'PSNR_Y': [],\n",
    "    'SSIM': [],\n",
    "    'VMAF': [],\n",
    "    'CAMBI': []\n",
    "    }\n",
    "\n",
    "    for idx, _row in SAMPLE.iterrows():\n",
    "        # get height/width of original video\n",
    "        _height = _row['Height']\n",
    "        _width = _row['Width']\n",
    "        _comp = os.path.join(baseCompPath,'res_'+_row['Comp'])\n",
    "        _ref = os.path.join(baseRefPath, _row['Ref'])\n",
    "        #print(_comp)\n",
    "        runTerminalCmd(f\"vmaf -r {_ref} -d {_comp} -w {_width} -h {_height} -p 420 -b 8 --csv -o processing/evals.csv --feature psnr --feature float_ssim --feature cambi -q\")\n",
    "        _cambi = pd.read_csv('processing/evals.csv')['cambi'].mean()\n",
    "        _psnr_y = pd.read_csv('processing/evals.csv')['psnr_y'].mean()\n",
    "        _psnr_cb = pd.read_csv('processing/evals.csv')['psnr_cb'].mean()\n",
    "        _psnr_cr = pd.read_csv('processing/evals.csv')['psnr_cr'].mean()\n",
    "        _ssim = pd.read_csv('processing/evals.csv')['float_ssim'].mean()\n",
    "        _vmaf = pd.read_csv('processing/evals.csv')['vmaf'].mean()\n",
    "        df_peralpha['Ref'].append(_row['Ref'])\n",
    "        df_peralpha['Comp'].append('res_'+_row['Comp'])\n",
    "        df_peralpha['Height'].append(_height)\n",
    "        df_peralpha['Width'].append(_width)\n",
    "        df_peralpha['PSNR_Y'].append(_psnr_y)\n",
    "        df_peralpha['PSNR_Cb'].append(_psnr_cb)\n",
    "        df_peralpha['PSNR_Cr'].append(_psnr_cr)\n",
    "        df_peralpha['SSIM'].append(_ssim)\n",
    "        df_peralpha['VMAF'].append(_vmaf)\n",
    "        df_peralpha['CAMBI'].append(_cambi)\n",
    "    df_peralpha = pd.DataFrame(df_peralpha)\n",
    "    df_peralpha.to_csv(f'alpha{_alpha}/alphaScores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
